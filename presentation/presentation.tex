\documentclass{beamer}
% \documentclass[aspectratio=169]{beamer} % 16:9 widescreen

\usepackage{hyperref}
\usepackage[T1]{fontenc}

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{biblatex}
\usepackage{graphicx,pstricks,listings,stackengine}

\addbibresource{refs.bib}

% \author{Santosh Desai, and Matt McAnear, Urvi Mehta}
\author{Santosh Desai, Matt McAnear, Urvi Mehta}
\title{Statistical Efficiency of Birth-Death Tree Parameter Estimation}
\subtitle{Stat 700}
\institute{Department of Statistics, University of Michigan}
\date{\today}

% Remove this if you don't have a custom CNU.sty theme
% \usepackage{CNU}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}


\begin{frame}
\maketitle
\end{frame}


\section{Background}

\begin{frame}{The Phylodeep Package}
    Phylodeep Paper is byt J. Voznica et al \cite{voznica_deep_2022}
    \begin{itemize}
        \item Parameter estimation is difficult and MLE method can be challenging for larger trees.
        \item Major components of the paper to address this
        \begin{itemize}
            \item CBLV representation of trees
            \item Pre-trained neural networks for pre-specified tree types
        \end{itemize}
        \item The goal is to speed up inference
    \end{itemize}
\end{frame}


\begin{frame}{Usage}
    \begin{itemize}
        \item Input: Phylogenetic tree in Newick format
        \item Output: Estimated parameters (e.g., birth rate ($\lambda$), death rate ($\mu$), $R_0$)
        \item Given a tree, Phylodeep selects the pre-trained model with the highest probability
        \item Outputs actually come with uncertainty estimates from a parametric bootstrap
    \end{itemize}
\end{frame}


\section{Methods}

\subsection{MLE}
\begin{frame}{Maximum Likelihood Estimation: Overview}
    \begin{itemize}
        \item \textbf{Goal}: Estimate birth rate ($\lambda$) and death rate ($\mu$) from phylogenetic trees
        \item \textbf{Method}: Numerical optimization of Stadler (2010)~\cite{stadler_sampling-through-time_2010} likelihood
        \item \textbf{Key Feature}: Works with \textbf{tree data only} - no nucleotide sequences required
        \item \textbf{Implementation}: Uses \texttt{scipy.optimize.minimize} with L-BFGS-B algorithm
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Advantages}:
    \begin{itemize}
        \item Statistically principled (maximum likelihood)
        \item Works on small trees (no minimum tip size requirement)
        \item Provides point estimates with optimization diagnostics
    \end{itemize}
\end{frame}

\begin{frame}{Stadler (2010) Likelihood Formulation}
    For a constant-rate birth-death process with sampling probability $\rho$:
    
    \begin{align*}
        \ell(\lambda, \mu | T, \rho) &= (n-1) \log(\lambda) \\
        &\quad - (\lambda + \mu) T_{\text{total}} \\
        &\quad + n \log(\rho) \\
        &\quad - r \cdot T + \text{penalty terms}
    \end{align*}
    
    where:
    \begin{itemize}
        \item $n$ = number of tips
        \item $T_{\text{total}}$ = total branch length
        \item $T$ = tree height (time from root to present)
        \item $r = \lambda - \mu$ = net diversification rate
        \item $\rho$ = sampling probability
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Key insight}: Survival probability term ($-r \cdot T$) helps identify $\mu$ separately from $\lambda$
\end{frame}

\begin{frame}{MLE Implementation: Tree Statistics Extraction}
    \textbf{Extracted from each tree}:
    \begin{itemize}
        \item Number of tips: $n$
        \item Total branch length: $T_{\text{total}} = \sum_{e \in E} \ell(e)$
        \item Tree height: $T = \max_{v \in V} d(\text{root}, v)$
        \item Branching times: All internal node ages
        \item Mean branch length: $\bar{\ell}$
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Initial parameter estimates} (method-of-moments):
    \begin{align*}
        \hat{r} &= \frac{\log(n)}{T} \\
        \hat{\lambda} &= \frac{n-1}{T_{\text{total}}} \\
        \hat{\mu} &= \max(0.01, \hat{\lambda} - \hat{r})
    \end{align*}
    
    These serve as starting values for numerical optimization.
\end{frame}

\begin{frame}{MLE Implementation: Optimization}
    \textbf{Numerical optimization}:
    \begin{itemize}
        \item \textbf{Algorithm}: L-BFGS-B (bounded optimization)
        \item \textbf{Bounds}: $\lambda \in [0.01, 10.0]$, $\mu \in [0.01, 10.0]$
        \item \textbf{Constraint}: $\mu < \lambda$ (enforced via penalty)
        \item \textbf{Gradient}: Analytical gradient provided for faster convergence
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Gradient components}:
    \begin{align*}
        \frac{\partial \ell}{\partial \lambda} &= -\frac{n-1}{\lambda} + T_{\text{total}} + T \\
        \frac{\partial \ell}{\partial \mu} &= T_{\text{total}} - T + \text{penalty term}
    \end{align*}
    
    \vspace{0.2cm}
    \textbf{Regularization}: Penalty term prevents $\mu$ from collapsing to lower bound in large trees
\end{frame}

\begin{frame}{MLE Implementation: Key Features}
    \footnotesize
    \textbf{1. Survival Probability Term}
    \begin{itemize}
        \item Includes $-r \cdot T$ term to help identify $\mu$
        \item Accounts for probability of observing $n$ tips given $\lambda$ and $\mu$
        \item Critical for separating $\mu$ from $\lambda$ in the likelihood
    \end{itemize}
    
    \vspace{0.15cm}
    \textbf{2. Regularization for Large Trees}
    \begin{itemize}
        \item Penalty: $-0.1 \log(\mu)$ when $\mu < 0.1$ and $n > 10$
        \item Prevents $\mu$ from getting stuck at lower bound (0.01)
        \item Encourages realistic death rate estimates based on tree size
    \end{itemize}
    
    \vspace{0.15cm}
    \textbf{3. Robust Error Handling}
    \begin{itemize}
        \item Checks for invalid trees ($n \leq 1$, $T_{\text{total}} \leq 0$)
        \item Handles optimization failures gracefully
        \item Returns success status, log-likelihood, and iteration count
    \end{itemize}
    
    \vspace{0.15cm}
    \textbf{4. Method-of-Moments Initialization}
    \begin{itemize}
        \item Smart starting values: $\hat{r} = \log(n)/T$, $\hat{\lambda} = (n-1)/T_{\text{total}}$
        \item Ensures faster convergence and avoids local minima
    \end{itemize}
\end{frame}

\begin{frame}{MLE: Advantages and Limitations}
    \small
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Statistically principled (maximum likelihood)
        \item \textcolor{deepgreen}{$\checkmark$} No minimum tree size requirement
        \item \textcolor{deepgreen}{$\checkmark$} Works on small trees ($n < 50$)
        \item \textcolor{deepgreen}{$\checkmark$} Provides optimization diagnostics
        \item \textcolor{deepgreen}{$\checkmark$} Fast computation (seconds per tree)
        \item \textcolor{deepgreen}{$\checkmark$} Based on well-established Stadler (2010)~\cite{stadler_sampling-through-time_2010} theory
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} Higher RMSE than PhyloDeep on large trees
        \item \textcolor{deepred}{$\times$} Point estimates only (no uncertainty quantification)
        \item \textcolor{deepred}{$\times$} Requires careful initialization for convergence
        \item \textcolor{deepred}{$\times$} Can struggle with very large trees ($n > 500$)
    \end{itemize}
    
    \vspace{0.1cm}
    \textbf{Best use case}: Small to medium trees where statistical rigor is important
\end{frame}

\subsection{Phylodeep}
\begin{frame}{PhyloDeep: How We Used the Package}
    \textbf{Pre-trained Models Used}:
    \begin{itemize}
        \item \textbf{Model Type}: BD (Birth-Death) pre-trained models from PhyloDeep package
        \item \textbf{We ONLY estimated BD trees} - used PhyloDeep's BD models exclusively
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Model Selection by Tree Size}:
    \begin{itemize}
        \item \textbf{Medium trees} ($50 \leq n < 200$): PhyloDeep automatically uses BD models trained on medium-sized trees
        \item \textbf{Large trees} ($n \geq 200$): PhyloDeep automatically uses BD models trained on large trees
        \item \textbf{Automatic selection}: PhyloDeep's \texttt{paramdeep} function selects the appropriate model based on tree size
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Implementation}:
    \begin{itemize}
        \item Called \texttt{paramdeep(tree\_file, sampling\_prob=0.5, model='BD', vector\_representation='FULL')}
        \item Used FULL (CNN-based CBLV) representation, not SUMSTATS
        \item Set \texttt{ci\_computation=False} for point estimates only
    \end{itemize}
\end{frame}

\begin{frame}{PhyloDeep: Key Features}
    \textbf{1. Pre-trained BD Models}
    \begin{itemize}
        \item Used existing BD models trained on 3.9 million trees (Voznica et al.)
        \item Fast inference (milliseconds per tree)
        \item Automatic model selection by tree size
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{2. FULL Tree Representation}
    \begin{itemize}
        \item CNN-based CBLV representation
        \item Converts tree to fixed-size vector for neural network processing
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{3. Output}
    \begin{itemize}
        \item Point estimates: $\lambda$, $\mu$, $R_0$, infectious period
    \end{itemize}
\end{frame}

\begin{frame}{PhyloDeep: Advantages and Limitations}
    \textbf{Advantages}:
    \begin{itemize}
        \item \textcolor{deepgreen}{$\checkmark$} Lower RMSE than MLE on large trees ($n \geq 50$)
        \item \textcolor{deepgreen}{$\checkmark$} Extremely fast inference (milliseconds)
        \item \textcolor{deepgreen}{$\checkmark$} Trained on massive dataset (3.9M trees)
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Limitations}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} Requires minimum 50 tips (cannot handle small trees)
        \item \textcolor{deepred}{$\times$} Black-box model (less interpretable)
        \item \textcolor{deepred}{$\times$} Point estimates only in our implementation
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Best use case}: Large trees ($n \geq 50$) where speed and accuracy are priorities
\end{frame}

\begin{frame}{Method Comparison: PhyloDeep vs MLE}
    \footnotesize
    \begin{tabular}{l|p{3.5cm}|p{3.5cm}}
        \textbf{Feature} & \textbf{PhyloDeep} & \textbf{MLE} \\
        \hline
        \textbf{Approach} & Deep learning & Classical stats \\
        \textbf{Training} & Pre-trained (3.9M) & None \\
        \textbf{Optimization} & Neural network & Single L-BFGS-B \\
        \textbf{Min Size} & 50 tips & None \\
        \textbf{Speed} & Milliseconds & Seconds \\
        \textbf{Accuracy} & Best (large trees) & Baseline \\
        \textbf{Interpretable} & No & Yes \\
    \end{tabular}
    
    \vspace{0.2cm}
    \textbf{When to Use}:
    \begin{itemize}
        \item \textbf{Small ($n < 50$)}: MLE only (PhyloDeep unavailable)
        \item \textbf{Medium ($50 \leq n < 200$)}: PhyloDeep (speed) or MLE (interpretability)
        \item \textbf{Large ($n \geq 200$)}: PhyloDeep (accuracy)
    \end{itemize}
\end{frame}

\section{Results}

\subsection{RMSE}

\begin{frame}
    \begin{figure}
        \includegraphics[width=1.0\linewidth]{images/RMSE_vs_tips.png}
        \caption[Figure 1]{Rolling bootstrapped RMSE estimates of $\mu$ and $\lambda$ by tree size and resulting 95\% CI. Window size is 500.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=1.0\linewidth]{images/estimation_error_vs_tips.png}
        \caption[Short Title]{Raw plot of the absolute errors in $\lambda, \mu$ by tree size.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=0.8\linewidth]{images/lambda_vs_mu_estimates.png}
        \caption{Plot of $\lambda$ vs $\mu$ estimates to show distribution of estimates.}
    \end{figure}
\end{frame}

\subsection{Bayesian MCMC}
\begin{frame}{Bayesian MCMC: Attempted Analysis}
    \footnotesize
    \textbf{What We Attempted}:
    \begin{itemize}
        \item Implemented Bayesian MCMC using PyMC for birth-death parameter estimation
        \item Used same Stadler (2010)~\cite{stadler_sampling-through-time_2010} likelihood as MLE
        \item Sequential processing to avoid PyTensor cache conflicts
        \item Goal: Full posterior distributions with uncertainty quantification
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Initial Results}:
    \begin{itemize}
        \item Successfully analyzed 3,550 trees (out of 8,655 total)
        \item Tree size range: 10-181 tips
        \item Success rate: 99.4\% (3,530/3,550 trees)
    \end{itemize}
    
    \vspace{0.2cm}
    \textbf{Why We Did Not Pursue Further}:
    \begin{itemize}
        \item \textcolor{deepred}{$\times$} RMSE was not better than MLE or PhyloDeep
        \item \textcolor{deepred}{$\times$} Computationally intensive (minutes per tree)
        \item \textcolor{deepred}{$\times$} Sequential processing too slow for full dataset
        \item \textcolor{deepred}{$\times$} No clear advantage to justify computational cost
    \end{itemize}
    
    \vspace{0.15cm}
    \textbf{Conclusion}: Bayesian MCMC provides uncertainty quantification but does not improve RMSE, so we focused on MLE and PhyloDeep for the main analysis.
\end{frame}

\begin{frame}{Results: Summary}
    \textbf{Key Findings}:
    \begin{itemize}
        \item RMSE decreases with increasing tree size for both methods
        \item \textbf{MLE}: Works on small trees ($n < 50$) where PhyloDeep fails
        \item \textbf{PhyloDeep}: Lower RMSE on average at all tree sizes where both methods apply ($n \geq 50$)
        \item \textbf{Bayesian MCMC}: Attempted but RMSE not better than MLE/PhyloDeep
    \end{itemize}
\end{frame}

\begin{frame}{Results: Method Comparison}
    \textbf{By Tree Size}:
    \begin{itemize}
        \item \textbf{Small trees ($n < 50$)}: MLE only (PhyloDeep unavailable)
        \item \textbf{Medium trees ($50 \leq n < 200$)}: PhyloDeep has lower RMSE on average
        \item \textbf{Large trees ($n \geq 200$)}: PhyloDeep has lower RMSE on average
    \end{itemize}
    
    \vspace{0.3cm}
    \textbf{Statistical Efficiency}:
    \begin{itemize}
        \item PhyloDeep's advantage: training on 3.9M trees enables better accuracy on average at all tree sizes
        \item MLE: statistically principled baseline, works on all tree sizes including small trees
        \item Bayesian MCMC: Attempted but did not improve RMSE, so not pursued further
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}{References}
    \printbibliography
\end{frame}

\end{document}
